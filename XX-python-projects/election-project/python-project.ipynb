{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Project\n",
    "\n",
    "In this part of the course we will use Python to predict polls as outcomes of the 2016 Prsidential Election. Sound's fun? \n",
    "\n",
    "Here's what we're going to do:\n",
    "1. use selenium to scrape search volume data from Google Trends\n",
    "3. use pandas to prepare the data\n",
    "2. use data.world to download 538 data from polls in each state\n",
    "4. use scipy to do some data analysis\n",
    "5. use XXX to visualize our results\n",
    "\n",
    "Let's go!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Web scraping\n",
    "Before starting to code your brains out, it's worth taking a look at [Google Trends](https://trends.google.com/trends). Familiarize yourself with the basic functionalities of the website. Search for something and try to get the data in csv form.\n",
    "\n",
    "Some things to think about:\n",
    "- Google Trends normalizes the data so that the peak search interest in the results corresponds to a value of 100. You can't unnormalize the data, but maybe it can actually save you time! Ask: How would I normalize anyways?\n",
    "- With this in mind, should you search for both candidates simultaneously or for one at a time?\n",
    "- There are different options to search: search terms and concepts. Which should you use?\n",
    "- We want to have a time series for each state. There two main ways to accomplish this: export the time series for each state or export the data in the map for each time point. Which should you use?\n",
    "- can you use a static method or will you have to used a dynamic webdriver?\n",
    "\n",
    "For this step-by-step guide we will compare both search terms simultaneously and export the map data for every day in the year leading up to the election. If the volume is very low, Google doesn't report anything. Although there exists a [workaround](https://www.sciencedirect.com/science/article/pii/S0047272714000929) we won't bother with it here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first step, write write a function `open_driver()` that imports and opens the selenium webdriver. Return the webdriver as an object. I suggest that you also import `By` and `Options`. \n",
    "\n",
    "You could try changing the download location using Options, but it might not work on your OS. We'll rename the downloads anyways so you can just move them to the correct location at that step. Don't worry, by now that should't be a challenge for you!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def open_driver():\n",
    "    \"\"\"\n",
    "    Function to open the chrome webdriver.\n",
    "    \"\"\"\n",
    "    # ---\n",
    "    # add your code here\n",
    "\n",
    "    \n",
    "    \n",
    "    # ---\n",
    "    print('Chrome driver is good to go!')\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to test your function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "driver = open_driver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, set up a example search for both candidates on election day (Nov 8, 2016). Take a look at your URL. While the date menue is relatively complicated to navigate, changing the date through the url id straightforward, so that is what we're going to do.\n",
    "https://trends.google.com/trends/explore?hl=en&date=[START_DATE]%20[END_DATE]&geo=[REGION]&q=[SEARCH_TERMS]\n",
    "\n",
    "Note: I added `hl=en` flag to the url, otherwise the labels might not be in English and you'll have trouble matching the data.\n",
    "\n",
    "Write a function that takes a start date, end date, region, and a list of search terms as inputs and returns the url. For fancy pants: Check if `searchterms` is a list and join it using commas if necessary. Replace spaces in search terms by `%20`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_url(start, end, region, searchterms):\n",
    "    \"\"\"\n",
    "    Function to construct the URL.\n",
    "    \"\"\"\n",
    "    # ---\n",
    "    # add your code here\n",
    "    \n",
    "\n",
    "    \n",
    "    # ---\n",
    "    return url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = build_url('2016-11-08','2016-11-08','US',['Donald Trump','Hillary Clinton'])\n",
    "url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now comes the tricky part: Write a function that opens the url in the driver and downloads the csv file for the map. Be careful, as there are multiple download buttons!\n",
    "\n",
    "Note: Google has a nasty habit of producting an error the first time you access the url. A relatively reliable work around uses the `time.sleep()` function to wait two seconds and try again. A more sophistiated solution would check if the download button is there and reload periodically until it is (although in my experience, either you get the page on the second attempt or you don't get it anytime soon).\n",
    "\n",
    "Also: All the files will have the same name when downloaded. This can cause some problems, expecially if you start a new download while there's still a previous file in the directory. I suggest that you first remove the old file (if it exists) and that you wait at the end until your download is complete. Both can easily be achieved using an `if os.path.exists(\"path/to/your/file\"):` clause."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def download_csv(url, driver):\n",
    "    \"\"\"\n",
    "    Function to download the csv file of the map.\n",
    "    \"\"\"\n",
    "    # ---\n",
    "    # add your code here\n",
    "    \n",
    "    \n",
    "    \n",
    "    # ---\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "download_csv(url, driver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to rename (and possibly move) the downloaded csv file. Name the file `\"map_[searchterms]_[startdate]_[enddate]_[region].csv\"`, to avoid accidentally overwriting existing files if you later explore other search specifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rename_csv(start, end, region, searchterms):\n",
    "    \"\"\"\n",
    "    Function to rename and move files.\n",
    "    \"\"\"\n",
    "    # ---\n",
    "    # add your code here\n",
    "    \n",
    "    \n",
    "    \n",
    "    # ---\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rename_csv('2016-11-08','2016-11-08','US',['Donald Trump','Hillary Clinton'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need a list of dates to loop over. There are many ways to do this. The internet is your friend here!\n",
    "\n",
    "Note: I suggest you only scrape a month's worth of data and copy the rest from the github repo as webscraping is time consuming and not universally appreciated (e.g. by Google's system admins)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_dates():\n",
    "    \"\"\"\n",
    "    Function to produce a list of dates with YYYY-MM-DD from 2015-11-08 to 2016-11-08.\n",
    "    \"\"\"\n",
    "    # ---\n",
    "    # add your code here\n",
    "    \n",
    "    \n",
    "    \n",
    "    # ---\n",
    "    return dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One last test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "get_dates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations, we're ready to put everything together!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    driver = open_driver()\n",
    "    searchterms = ['Donald Trump','Hillary Clinton']\n",
    "    region = 'US'\n",
    "    dates = get_dates()\n",
    "    for date in dates:\n",
    "        print('Download data for: ', date)\n",
    "        url = build_url(date, date, region, searchterms)\n",
    "        download_csv(url, driver)\n",
    "        rename_csv(date, date, region, searchterms)\n",
    "    driver.quit()\n",
    "    print('All data downloaded.')\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "  main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare Google Trends data\n",
    "\n",
    "Now that we have the data from Google, we're going to combine all the csv files in a large pandas dataframe. \n",
    "\n",
    "- use `os.listdir()` to generate a list of all the files\n",
    "- write a loop to append all of them in a large pandas dataframe\n",
    "- clean the data to remove missing values and normalize the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, use `os.listdir()` to generate a list `files` of all the files we want to load. If you saved other data in the same directory, you might need to filter the list and keep just the Google Trends data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ---\n",
    "# add your code here\n",
    "\n",
    "\n",
    "\n",
    "# ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If done correctly, you should have 367 files (Nov 8th is included twice and 2016 was a leap year). Let's see how many we got:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function `load_data()` to load a file from into a pandas dataframe with colums `state`, `trump`, and `clinton`, and generate a column 'date' that contains the date from the file name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(f):\n",
    "    # ---\n",
    "    # add your code here\n",
    "    \n",
    "    \n",
    "    \n",
    "    # ---\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "load_data(files[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an empty pandas dataframe `google_raw` and write a loop over `files` that applies `load_data()` on each file and appends it to `google_raw`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ---\n",
    "# add your code here\n",
    "\n",
    "\n",
    "\n",
    "# ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see whether it worked:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "google_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll do some housekeeping:\n",
    "\n",
    "1. create a copy of the 'google_raw' dataframe so that we don't have to rerun the loop if we mess up.\n",
    "2. remove missing values\n",
    "3. remove the `%` symbol from the percentages\n",
    "4. normalize the data such that `candidate = candidate / (total searches)`\n",
    "5. use `groupby` to order by `date` and `state`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ---\n",
    "# add your code here\n",
    "\n",
    "\n",
    "\n",
    "# ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the result and copying it to `google_data`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "google_data = df.copy()\n",
    "google_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare polling data\n",
    "\n",
    "We're going to use polling data compiled by 538. However, instead of scraping that data too, we're going to use a short cut. Interesting data has often been scraped by somebody else before, so you can save a lot of time by googling datasets and checking github and other data repositories!\n",
    "\n",
    "1. Go to [data.world](data.world) and registre as a new user (unless you're an old user, duh!). They have some cool data, so it's worth it.\n",
    "2. Download [presidential_polls_2016_fivethirtyeight.csv](https://data.world/databeats/2016-us-presidential-election/workspace/file?filename=presidential_polls_2016_fivethirtyeight.csv) to an appropriate location.\n",
    "\n",
    "Note: If you feel like showing off, try creating an API key and downloading the data using the python module. [Here's some guidance.](https://data.world/integrations/python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the poll data into a pandas dataframe called `polls`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ---\n",
    "# add your code here\n",
    "\n",
    "\n",
    "\n",
    "# ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "polls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure that the data are comparable, check whether all the results are for the same election cycle, branch of government, type, forecastdate, and most importantly, the same matchup of candidates! Table the data to make sure that this is indead the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ---\n",
    "# add your code here\n",
    "\n",
    "\n",
    "\n",
    "# ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems mostly ok. But what's up with type? Turns out that each poll is included three times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "polls.groupby('poll_id')['cycle'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll have to change that. The next step is filtering the data!\n",
    "\n",
    "Use pandas amazing data-slizing ability to generate `polls_filtered` such that: \n",
    "- only `polls-only` are included\n",
    "- only polls with information on sample size are included\n",
    "- national polls (`state` = 'U.S.') are excluded\n",
    "- Main and Nebraska have split electoral votes. Make sure to include only polls for the entire state. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ---\n",
    "# add your code here\n",
    "\n",
    "\n",
    "\n",
    "# ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "polls_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we have to aggregate the data so that we can merge it on the day and state. There are many possibilities and there's going to be an element of subjective judgement. Here's one way:\n",
    "\n",
    "- `date`: There's a start date and an end date. We deal with this by calculating the number of days that the poll run for and assuming that the same number of repondents participated on each day.\n",
    "- `state`: There can be multiple polls in the same state and on the same day. We deal with this by calculating a weighted average based on the (estimated) sample size of each poll on that date.\n",
    "\n",
    "Step by step:\n",
    "1. copy the `polls_filtere` to a new dataframe `df`. We don't want to make a mess!\n",
    "2. calculate the number of days that the poll ran for from `startdate` and `enddate`.\n",
    "3. calculate `samplesize_day` by dividing `samplesize` by the number of days.\n",
    "4. expand the dataframe such that each observation is included `days` time.\n",
    "5. generate a variable `date` with the ficticious polling date.\n",
    "6. aggregate the data by date and state such that `rawpoll_clinton`, `rawpoll_trump`, `adjpoll_clinton` and `adjpoll_trump` are averages weighted by the sample size, and `samplesize` is the sum of the daily sample sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ---\n",
    "# add your code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "poll_data = df.copy()\n",
    "poll_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analyze data\n",
    "With our data at hand, we can finally test what google searches reveal about political opinion. We combine the data (easy!) and use the scipy module to test some ideas. After that you're free to explore!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine Google and 538 data using `pd.concat()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ---\n",
    "# add your code here\n",
    "\n",
    "\n",
    "\n",
    "# ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always, check the result! (If you have no complete observations, something went wrong with the matching. Most likely the date wasn't the same format.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we use scipy, it's worth keeping in mind that pandas has a lot of data science capability built in! Try to estimate the correlation between the relative search volume for Trump and his performance in the polls using `.corr()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ---\n",
    "# add your code here\n",
    "\n",
    "\n",
    "\n",
    "# ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like people didn't particularily like the candidates they googled..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some additional ideas to test if you have enough time:\n",
    "- Search data around the primaries might be missleading. Does the fit improve if we only look after the national conventions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
