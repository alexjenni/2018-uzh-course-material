{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: Linear Models + Regularization\n",
    "## 1. Estimate an Elastic Net model\n",
    "In this exercise we estimate elastic net models with $\\alpha \\in \\{0.25,0.5,0.75\\}$.\n",
    "\n",
    "Use CV (`cv.glmnet`) to choose the $\\lambda$ minimizing the CV test error for each $\\alpha$.\n",
    "Calculate and show the minimal test error for each $\\alpha$.\n",
    "\n",
    "The following code will get you started:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: Matrix\n",
      "Loading required package: foreach\n",
      "Loaded glmnet 2.0-16\n",
      "\n",
      "Loading required package: lattice\n",
      "Loading required package: ggplot2\n"
     ]
    }
   ],
   "source": [
    "library(mlbench)\n",
    "library(glmnet)\n",
    "library(caret)\n",
    "data(BostonHousing)\n",
    "trainIndex <- createDataPartition(BostonHousing$medv,p=0.5,list=FALSE)\n",
    "training.data <- BostonHousing[trainIndex,]\n",
    "test.data <- BostonHousing[-trainIndex,]\n",
    "x <- model.matrix(medv~.,data=training.data)[,-1]\n",
    "y <- training.data$medv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Compare the optimal ElasticNet model to OLS using our hold-out sample (test.data)\n",
    "Which is doing better? OLS or Elastic NET?\n",
    "\n",
    "Hint: recall that we got the OLS fit like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ols.fit <- lm(medv~.,data=training.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. True Power of Regularization\n",
    "The improvement in the test error from using Elastic NET was quite minimal before.\n",
    "The reason is that the models were actually not that complex, and most variables add enough predictive information to warrant them being included.\n",
    "OLS did not overfit a lot.\n",
    "\n",
    "Next, we are going to construct a more complex linear model. Let us add all interaction terms to our model. In R that's as easy as this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "frm <- medv ~ (age + b + chas + crim + dis + indus + lstat + nox + ptratio + rad + rm + tax + zn)^2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a.** Fit an unregularized OLS model for the `frm` formula. What is the average test error on `test.data`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b.** Now fit an Elastic Net model for `frm` with $\\alpha=0.5$. Compare the minimal test error with OLS and the previous model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c.** Plot the test error as a function of $\\lambda$. How many variables are included at the optimal $\\lambda$? How many are discarded?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
